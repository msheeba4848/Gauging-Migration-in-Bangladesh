{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Data 2\"\n",
        "format: html\n",
        "editor: visual\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{r}\n",
        "library(caret)\n",
        "df_original <- read.csv('../cleaned_data/cleaning_data-02.csv')\n",
        "#df <- replace(df, is.na(df), 99999)\n",
        "#df <- df[, c(-1,-2,-3,-4,-9)]\n",
        "#df <- as.data.frame(lapply(df, as.factor))\n",
        "#print(head(df))\n",
        "\n",
        "```\n",
        "\n",
        "# Model 1\n",
        "\n",
        "Independent Variable D1A_1V1L: Internal: Primary purpose of trip: work/earn money - Last, Head\n",
        "\n",
        "Dependent Variable A12: Household: Can you write a letter? A13: Household: Level of education (Highest level passed) A14: Household: Livelihood/occupation D1A_6AL: Internal: District of destination - Last, Head D1A_7ML: Internal: Month of arrival - Last, Head A15V1: Household: Have migration experience?: Internal -- not in cleaned data? A15V2: Household: Have migration experience?: India -- not in cleaned data? A15V3: Household: Have migration experience?: Other country A15V4: Household: Have migration experience?: No migration\n",
        "\n",
        "```{r}\n",
        "df1 <-df_original[, c(\"D1A_1V1L\", \"A12\", \"A13\", \"A14\", \"D1A_7ML\", \"A15V1\", \"A15V4\")]\n",
        "# temporarily taking out D1A_6AL\n",
        "\n",
        "#converting cateogorical variables to factor\n",
        "df1 <- as.data.frame(lapply(df1, factor))\n",
        "print(head(df1))\n",
        "```\n",
        "\n",
        "```{r}\n",
        "# converting variables to dummy variables\n",
        "library(\"fastDummies\")\n",
        "df1 <- fastDummies::dummy_cols(df1)\n",
        "print(dim(df1))\n",
        "```\n",
        "\n",
        "```{r}\n",
        "# removing rows that do not have D1A_1V1L\n",
        "df1 <- df1[!is.na(df1$D1A_1V1L), ]\n",
        "# Dropped duplicate columns that we made into dummy variables already\n",
        "df1 <- df1[, -which(names(df1) %in% c(\"D1A_1V1L_1\", \"D1A_1V1L_0\", \"D1A_1V1L_NA\", \"A12_2\", \"A12\", \"A13\", \"A14\", \"D1A_7ML\", \"A15V1\", \"A15V4\"))]\n",
        "#replace NA with 0, 0 meaning \"No\", assuming that if the participant did not say yes to a question, than it would be no.\n",
        "df1 <- replace(df1, is.na(df1), 0)\n",
        "print(head(df1))\n",
        "\n",
        "```\n",
        "\n",
        "```{r}\n",
        "# INSERT CODE \n",
        "\n",
        "set.seed(123) # for reproducibility\n",
        "train_idx <- createDataPartition(df1$D1A_1V1L, p = 0.8, list = FALSE)\n",
        "train <- df1[train_idx,]\n",
        "test <- df1[-train_idx,] \n",
        "```\n",
        "\n",
        "```{r}\n",
        "\n",
        "#trying a logistic regression\n",
        "model <- glm(D1A_1V1L ~ ., data = df1, family = binomial)\n",
        "\n",
        "# look at summary of logistic regression model\n",
        "(summary2_lm <- summary(model))\n",
        "```\n",
        "\n",
        "```{r}\n",
        "\n",
        "# Create model with predictors found from linear model\n",
        "model2 <- glm(D1A_1V1L ~ A14_8 + A14_9 + A14_17 + D1A_7ML_9, data = train, family = binomial)\n",
        "\n",
        "# Make predictions on test data\n",
        "predictions <- predict(model2, newdata = test, type = \"response\")\n",
        "\n",
        "# Convert probabilities to class labels, assign greater than 0.5 to Positive\n",
        "pred_classes <- ifelse(predictions > 0.5, \"Positive\", \"Negative\")\n",
        "\n",
        "# Create the confusion matrix\n",
        "cm <- table(Actual = test$D1A_1V1L, Predicted = pred_classes)\n",
        "\n",
        "# Confusion Matrix\n",
        "print(cm)\n",
        "```\n",
        "\n",
        "```{r}\n",
        "# Calculate True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN)\n",
        "TP <- 333\n",
        "TN <- 20\n",
        "FP <- 43\n",
        "FN <- 3\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy <- (TP + TN) / sum(cm)\n",
        "\n",
        "# Calculate precision\n",
        "precision <- TP / (TP + FP)\n",
        "\n",
        "# Calculate recall (also called sensitivity)\n",
        "recall <- TP / (TP + FN)\n",
        "\n",
        "# Calculate F1 score\n",
        "F1 <- 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "# Print the metrics\n",
        "cat(\"Accuracy:\", accuracy, \"\\n\")\n",
        "cat(\"Precision:\", precision, \"\\n\")\n",
        "cat(\"Recall:\", recall, \"\\n\")\n",
        "cat(\"F1 Score:\", F1, \"\\n\")\n",
        "\n",
        "```\n",
        "\n",
        "## Regsubset - Attempt\n",
        "\n",
        "```{r}\n",
        "# INSERT CODE \n",
        "library(leaps)\n",
        "\n",
        "model1 <- regsubsets(D1A_1V1L ~ A14_8 + A14_9 + A14_17 + D1A_7ML_9, data = df1, nvmax = 71, method = \"exhaustive\")\n",
        "\n",
        "summary(model1)\n",
        "\n",
        "results1 <- summary(model1)\n",
        "\n",
        "results1$adjr2\n",
        "results1$cp\n",
        "results1$bic\n",
        "\n",
        "\n",
        "data.frame(\n",
        "  Adj.R2 = which.max(results1$adjr2),\n",
        "  CP = which.min(results1$cp),\n",
        "  BIC = which.min(results1$bic)\n",
        ")\n",
        "```\n",
        "\n",
        "## Feature Selection/Interesting Findings\n",
        "\n",
        "Results: A14_8 -1.747e+00 8.781e-01 -1.989 0.0467 \\*\\\n",
        "A14_9 2.309e+00 9.075e-01 2.544 0.0110 \\* A14_17 -2.934e+00 6.180e-01 -4.747 2.06e-06 \\*\\* *D1A_7ML_9 1.268e+00 6.034e-01 2.101 0.0356*\n",
        "\n",
        "Descriptions Mapped: A14_8: Domestic servant A14_9: Non agricultural worker(factory worker, blue collar service) A14_17: Homemaker D1A_7ML_9: D1A_7ML: Internal: Month of arrival - Last, Head - Month 9/September \\*\\*\\* could we also do year??\n",
        "\n",
        "Conclusion: These predictors may have a relationship to D1A_1V1L: Internal: Primary purpose of trip: work/earn money - Last, Head.\n",
        "\n",
        "# Model 2\n",
        "\n",
        "Independent Variable D1A_1V1L: Internal: Primary purpose of trip: work/earn money - Last, Head\n",
        "\n",
        "Dependent Variable. -- numeric D1A_4: Internal: Total number of trips - Head.\\\n",
        "-- numeric A08: Household: Year of birth converted to Age.\\\n",
        "-- numeric N1_6TAKA: Internal: How much paid in taka?\\\n",
        "-- numeric A11Y: Household: Age at first marriage.\n",
        "\n",
        "N1_6TAKA. N1_12. N1_13. N1_14: Internal: Average monthly remittances sent home\\\n",
        "N1_16: Internal: Average monthly savings. N1_17\\\n",
        "G1_5A1SIL1\\\n",
        "G2_5B1S1 D1A_10AF_3M: Internal: Wage(taka)- First, Head Monthly\\\n",
        "D1A_10AL_3M. D1A_8F: Internal: Duration of stay - First, Head.\n",
        "\n",
        "```{r}\n",
        "df2 <-df_original[, c(\"D1A_1V1L\", \"D1A_4\", \"A08\", \"A11Y\" ,\"N1_6TAKA\", \"N1_12\", \"N1_13\", \"N1_14\", \"N1_16\", \"N1_17\", \"D1A_10AF_3M\", \"D1A_10AL_3M\", \"D1A_8F\")]\n",
        "print(head(df2))\n",
        "```\n",
        "\n",
        "```{r}\n",
        "df2 <- df2[!is.na(df2$D1A_1V1L), ]\n",
        "df2 <- na.omit(df2)\n",
        "df2$Age <- 2019 - df2$A08\n",
        "df2 <-df2[, c(\"D1A_1V1L\", \"D1A_4\", \"A11Y\" ,\"N1_6TAKA\", \"N1_12\", \"N1_13\", \"N1_14\", \"N1_16\", \"N1_17\", \"D1A_10AF_3M\", \"D1A_10AL_3M\", \"D1A_8F\")]\n",
        "print(head(df2))\n",
        "print(dim(df2))\n",
        "```\n",
        "\n",
        "```{r}\n",
        "model <- lm(D1A_1V1L ~ ., data = df2)\n",
        "\n",
        "# look at summary of linear model\n",
        "(summary2_lm <- summary(model))\n",
        "```\n",
        "\n",
        "## Feature Selection/Interesting Findings\n",
        "\n",
        "Conclusion: Number of trips predictor of primary purpose of trip to work/earn money\n",
        "\n",
        "Add D1C_7MF: Internal: Month of arrival - First, Other D1C_8F: Internal: Duration of stay - First, Other\n",
        "\n",
        "# Model 3\n",
        "\n",
        "Independent Variables D1A_1V9L: Internal: Primary purpose of trip: accompany family - Last, Head\n",
        "\n",
        "Dependent Variable A12: Household: Can you write a letter? A13: Household: Level of education (Highest level passed) A14: Household: Livelihood/occupation D1A_6AL: Internal: District of destination - Last, Head D1A_7ML: Internal: Month of arrival - Last, Head A15V1: Household: Have migration experience?: Internal -- not in cleaned data? A15V2: Household: Have migration experience?: India -- not in cleaned data? A15V3: Household: Have migration experience?: Other country A15V4: Household: Have migration experience?: No migration\n",
        "\n",
        "```{r}\n",
        "df3 <-df_original[, c(\"D1A_1V9L\", \"A12\", \"A13\", \"A14\", \"D1A_7ML\", \"A15V1\", \"A15V4\")]\n",
        "# temporarily taking out D1A_6AL\n",
        "df3 <- as.data.frame(lapply(df3, factor))\n",
        "print(head(df3))\n",
        "```\n",
        "\n",
        "```{r}\n",
        "library(\"fastDummies\")\n",
        "df3 <- fastDummies::dummy_cols(df3)\n",
        "print(dim(df3))\n",
        "```\n",
        "\n",
        "```{r}\n",
        "df3 <- df3[!is.na(df3$D1A_1V9L), ]\n",
        "# Drop specified columns from df2\n",
        "df3 <- df3[, -which(names(df3) %in% c(\"D1A_1V9L_1\", \"D1A_1V9L_0\", \"D1A_1V9L_NA\", \"A12_2\", \"A12\", \"A13\", \"A14\", \"D1A_7ML\", \"A15V1\", \"A15V4\"))]\n",
        "df3 <- replace(df3, is.na(df3), 0)\n",
        "print(head(df2))\n",
        "```\n",
        "\n",
        "```{r}\n",
        "model <- glm(D1A_1V9L ~ ., data = df3, family=binomial)\n",
        "\n",
        "# look at summary of linear model\n",
        "(summary2_lm <- summary(model))\n",
        "```\n",
        "\n",
        "## Feature Selection/Interesting Findings\n",
        "\n",
        "Not much different from Model 1\n",
        "\n",
        "```{r}\n",
        "\n",
        "#College <- na.omit(College)\n",
        "\n",
        "# fit data into matrix\n",
        "#x <- model.matrix(Apps~ ., College)[, ]\n",
        "#y <- College$Apps\n",
        "\n",
        "x <- model.matrix(D1A_1V1L ~ ., df1)[, ]\n",
        "y <- df1$D1A_1V1L\n",
        "\n",
        "# split data\n",
        "set.seed(1)\n",
        "train <- sample(c(TRUE, FALSE), nrow(df1), replace=TRUE, prob=c(0.8,0.2))\n",
        "test <- (-train)\n",
        "y.test <- y[test]\n",
        "\n",
        "#train <- data.matrix(train[, !names(train) %in% c(\"Apps\")])\n",
        "#test <- data.matrix(test[, !names(test) %in% c(\"Apps\")])\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "```{r}\n",
        "library(glmnet)\n",
        "set.seed(1)\n",
        "y <- as.numeric(as.character(y))\n",
        "ridge.mod <- glmnet(x[train, ], y[train], alpha = 0, thresh = 1e-12)\n",
        "#plot(ridge.mod)\n",
        "\n",
        "cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0, lambda.min.ratio=0.000001)\n",
        "# Plot the test MSE as a function of the log of the regularization \n",
        "# parameter (i.e. log($\\lambda$)) for several orders of magnitude\n",
        "# with error plotted\n",
        "plot(cv.out, main=\"MSE for Several Orders of Magnitude\")\n",
        "# from the plot, the error remains constant around 10, so i will choose 10 for lambda\n",
        "#cv.glmnet(x_train,y_train,alpha=0, lambda.min.ratio=0.000001)\n",
        "bestlam <- cv.out$lambda.min\n",
        "bestlam \n",
        "\n",
        "# best lambda is 10, can also evaluate plot to find best lambda\n",
        "# bestlam=10\n",
        "\n",
        "ridge.pred <- predict(ridge.mod, s = bestlam,\n",
        "    newx = x[test, ])\n",
        "\n",
        "# MSE\n",
        "ridge_mse <- mean((ridge.pred - y.test)^2)\n",
        "# RMSE\n",
        "ridge_rmse <- sqrt(ridge_mse)\n",
        "\n",
        "out <- glmnet(x, y, alpha = 0, lambda = bestlam) # Fit ridge regression model on full dataset\n",
        "predict(out, type = \"coefficients\", s = bestlam)[1:16,] # Display coefficients using lambda chosen by CV\n",
        "# coef\n",
        "coef(out)\n",
        "\n",
        "# report test error\n",
        "print(\"Report Test Error:\")\n",
        "cat(\"Ridge MSE:\", ridge_mse, \"\\n\")\n",
        "cat(\"Ridge RMSE:\", ridge_rmse, \"\\n\")\n",
        "print(\"The test error is greater with Ridge Regression compared to linear regression.\")\n",
        "\n",
        "```\n",
        "\n",
        "```{r}\n",
        "\n",
        "# Call lambda values and corresponding test MSEs\n",
        "lambda_values <- cv.out$lambda\n",
        "test_mses <- cv.out$cvm\n",
        "\n",
        "# Plot the test MSE as a function of the log of the regularization \n",
        "# parameter (i.e. log($\\lambda$)) for several orders of magnitude.\n",
        "\n",
        "plot(log(lambda_values), test_mses, type = \"b\", \n",
        "     xlab = \"log(lambda)\", ylab = \"Test MSE\",\n",
        "     main = \"Test MSE vs. log(lambda) of Ridge Regression\")\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "```{r}\n",
        "\n",
        "lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda.min.ratio=0.000001)\n",
        "\n",
        "# plot of non-zero coefficient estimates, baseline model\n",
        "\n",
        "#plot(lasso.mod) \\### set.seed(1) cv.out \\<- cv.glmnet(x\\[train, \\], y\\[train\\], alpha = 1, lambda.min.ratio=0.000001)\n",
        "\n",
        "# Plot the MSE as a function of the log of the\n",
        "\n",
        "# regularization parameter (i.e. log($\\lambda$)) for\n",
        "\n",
        "# several orders of magnitude.\n",
        "\n",
        "plot(cv.out)\n",
        "\n",
        "# find lambda that minimizes training MSE\n",
        "\n",
        "bestlam <- cv.out$lambda.min\n",
        "\n",
        "# Another option is to base off the plot. let's set to 7\n",
        "\n",
        "# bestlam \\<- 7\n",
        "\n",
        "# predict the test data with the best lambda\n",
        "\n",
        "lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test, ])\n",
        "\n",
        "# plot of non-zero coefficient estimates\n",
        "\n",
        "plot(lasso.mod, xvar=\"lambda\")\n",
        "\n",
        "# Calculate the test MSE\n",
        "\n",
        "lasso_mse <- mean((lasso.pred - y.test)^2) # Calculate the test RMSE lasso_rmse \\<- sqrt(mse)\n",
        "\n",
        "lasso.mod.final <- glmnet(x[train, ], y[train], alpha = 1, lambda=bestlam)\n",
        "\n",
        "# sparse matrix\n",
        "\n",
        "coef(lasso.mod.final)\n",
        "\n",
        "# report test error\n",
        "\n",
        "print(\"Report Test Error:\") \n",
        "cat(\"Lasso MSE:\", lasso_mse, \"\\n\") \n",
        "#cat(\"lasso RMSE:\", lasso_rmse, \"\\n\") \n",
        "print(\"The test error for Lasso Regression is less than Ridge Regression compared.\") \n",
        "print(\"Lasso Regression had a similar test error to linear regression.\")\n",
        "\n",
        "# plot of non-zero coefficient estimates\n",
        "\n",
        "#plot(lasso.mod)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#https://www.geeksforgeeks.org/random-forest-approach-in-r-programming/\n",
        "\n",
        "```{r}\n",
        "\n",
        "\n",
        "library(randomForest)\n",
        "\n",
        "\n",
        "# Fitting Random Forest to the train dataset\n",
        "set.seed(120)  # Setting seed\n",
        "classifier_RF <- randomForest(x[train, ], y[train],\n",
        "                               ntree = 500)\n",
        "\n",
        "# Printing the Random Forest model\n",
        "print(classifier_RF)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred <- predict(classifier_RF, newdata = x[test, ])\n",
        "\n",
        "\n",
        "# Confusion Matrix\n",
        "#confusion_mtx <- table(y[test], y_pred)\n",
        "#print(confusion_mtx)\n",
        "\n",
        "# Plotting the model\n",
        "plot(classifier_RF)\n",
        "\n",
        "# Importance plot\n",
        "#importance(classifier_RF)\n",
        "\n",
        "# Variable importance plot\n",
        "varImpPlot(classifier_RF)\n",
        "\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}