---
title: "Data 2"
format: html
editor: visual
---

```{r}
library(caret)
df_original <- read.csv('../cleaned_data/cleaning_data-02.csv')
#df <- replace(df, is.na(df), 99999)
#df <- df[, c(-1,-2,-3,-4,-9)]
#df <- as.data.frame(lapply(df, as.factor))
#print(head(df))

```

# Base Model: Logistic Regression

Independent Variable D1A_1V1L: Internal: Primary purpose of trip: work/earn money - Last, Head

Dependent Variable. A12: Household: Can you write a letter?\
A13: Household: Level of education (Highest level passed) A14: Household: Livelihood/occupation D1A_6AL: Internal: District of destination - Last, Head D1A_7ML: Internal: Month of arrival - Last, Head A15V1: Household: Have migration experience?: Internal -- not in cleaned data? A15V2: Household: Have migration experience?: India -- not in cleaned data? A15V3: Household: Have migration experience?: Other country A15V4: Household: Have migration experience?: No migration

```{r}
#df1 <-df_original[, c("D1A_1V1L", "A12", "A13", "A14", "D1A_7ML", "A15V1", "A15V4")]
df1 <-df_original[, c("D1A_1V1L", "A08", "A12", "A13", "A14", "D1A_7ML", "A15V1", "A15V4", "D1A_4", "A11Y" ,"N1_6TAKA", "N1_12", "N1_13", "N1_14", "N1_16", "N1_17", "D1A_10AF_3M", "D1A_10AL_3M", "D1A_8F")]
df1$Age <- 2019 - df1$A08
df1 <-df1[, c("D1A_1V1L", "A12", "A13", "A14", "D1A_7ML", "A15V1", "A15V4", "D1A_4", "A11Y" ,"N1_6TAKA", "N1_12", "N1_13", "N1_14", "N1_16", "N1_17", "D1A_10AF_3M", "D1A_10AL_3M", "D1A_8F")]
# temporarily taking out D1A_6AL

#converting cateogorical variables to factor
columns_to_factor <- c("D1A_1V1L", "A12", "A13", "A14", "D1A_7ML", "A15V1", "A15V4")
df1[columns_to_factor] <- lapply(df1[columns_to_factor], factor)

#df1 <- as.data.frame(lapply(df1, factor))
print(head(df1))
```

```{r}
# converting variables to dummy variables
library("fastDummies")
df1 <- fastDummies::dummy_cols(df1, remove_first_dummy = TRUE, remove_selected_columns = TRUE)
print(dim(df1))
```

```{r}
# removing rows that do not have D1A_1V1L
df1 <- df1[!is.na(df1$D1A_1V1L_1), ]
#df1 <- df1[!is.na(df1$D1A_1V1L), ]
# Dropped duplicate columns that we made into dummy variables already
#df1 <- df1[, -which(names(df1) %in% c("D1A_1V1L_1", "D1A_1V1L_0", "D1A_1V1L_NA", "A12_2", "A12", "A13", "A14", "D1A_7ML", "A15V1", "A15V4"))]
#replace NA with 0, 0 meaning "No", assuming that if the participant did not say yes to a question, than it would be no.
df1 <- replace(df1, is.na(df1), 0)
print(head(df1))

```

```{r}
# INSERT CODE 

set.seed(123) # for reproducibility
#train_idx <- createDataPartition(df1$D1A_1V1L, p = 0.8, list = FALSE)
train_idx <- createDataPartition(df1$D1A_1V1L_1, p = 0.8, list = FALSE)
train <- df1[train_idx,]
test <- df1[-train_idx,] 
```

```{r}

#trying a logistic regression
#model <- glm(D1A_1V1L ~ ., data = df1, family = binomial(link='logit'))
model <- glm(D1A_1V1L_1 ~ ., data = df1, family = binomial(link='logit'))

# look at summary of logistic regression model
(summary2_lm <- summary(model))
```

```{r}

# Create model with predictors found from linear model
model2 <- glm(D1A_1V1L_1 ~ D1A_4 + N1_12 + N1_14 + D1A_10AL_3M + D1A_8F + A13_3 + A13_5 + 
                A13_6 + A13_7 + A13_8 + A14_7 + A14_9 + A14_17, data = train, family = binomial)

# Make predictions on test data
predictions <- predict(model2, newdata = test, type = "response")

# Convert probabilities to class labels, assign greater than 0.5 to Positive
pred_classes <- ifelse(predictions > 0.5, "Positive", "Negative")

# Create the confusion matrix
cm <- table(Actual = test$D1A_1V1L_1, Predicted = pred_classes)

# Confusion Matrix
print(cm)
```

```{r}
# Calculate True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN)
TP <- 333
TN <- 20
FP <- 43
FN <- 3

# Calculate accuracy
accuracy <- (TP + TN) / sum(cm)

# Calculate precision
precision <- TP / (TP + FP)

# Calculate recall (also called sensitivity)
recall <- TP / (TP + FN)

# Calculate F1 score
F1 <- 2 * (precision * recall) / (precision + recall)

# Print the metrics
cat("Accuracy:", accuracy, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1 Score:", F1, "\n")

```

## Feature Selection/Interesting Findings

## Final Conclusions

The following are features are predictors of D1A_1V1L: Internal: Primary purpose of trip: work/earn money - Last, Head.

D1A_4 2.102e-01 8.233e-02 2.553 0.010670 - D1A_4: Internal: Total number of trips - Head

N1_12 1.868e-04 6.995e-05 2.670 0.007590 - N1_12: Internal: Rent per month

N1_14 3.018e-04 4.956e-05 6.089 1.13e-09 \*\*\* -\> N1_14: Internal: Average monthly remittances sent home

D1A_10AL_3M 1.371e-04 3.409e-05 4.021 5.80e-05\*\*\* -\> D1A_10AL_3M: Internal: Wage(taka)- Last, Head Monthly

D1A_8F -1.485e-03 2.789e-04 -5.326 1.01e-07 \*\*\* -\> D1A_8F: Internal: Duration of stay - First, Head

A13_3 -1.055e+00 4.906e-01 -2.150 0.031541\*\*\* \*\*-\> A13: Household: Level of education (Highest level passed) - 3 Class V (Complete PE)

A13_5 -1.523e+00 5.347e-01 -2.849 0.004385\*\* -\> A13: Household: Level of education (Highest level passed) - 5 SSC (Complete SE)

A13_6 -1.947e+00 6.565e-01 -2.965 0.003025 -\> A13: Household: Level of education (Highest level passed) - 6 College (11 and 12 grades)

A13_7 -1.826e+00 5.520e-01 -3.309 0.000936 -\> A13: Household: Level of education (Highest level passed) - 7 HSC (complete HSE)

A13_8 -2.470e+00 5.359e-01 -4.609 4.04e-06 -\> A13: Household: Level of education (Highest level passed) - 8 University level

A14_7 1.500e+00 7.491e-01 2.003 0.045199 -\> A14: Household: Livelihood/occupation - 7 Rickshaw driver/ Brick breaking/Road building/Construction worker/boatman/earth

A14_9 2.427e+00 8.715e-01 2.785 0.005354 -\> 9 Non agricultural worker(factory worker, blue collar service)

A14_17 -2.769e+00 6.844e-01 -4.046 5.21e-05 -\> 17 Homemaker



# Ridge

```{r}
library(glmnet)
library(pROC)

# Load your data and prepare x and y
x <- model.matrix(D1A_1V1L_1 ~ ., df1)[, ]
y <- df1$D1A_1V1L_1

# Split data into train and test sets
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(df1), replace = TRUE, prob = c(0.8, 0.2))
test <- !train
y.test <- y[test]

# Train Ridge regression model
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0, lambda.min.ratio = 0.000001)

# Cross-validation for selecting lambda
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0, lambda.min.ratio = 0.000001)
bestlam <- cv.out$lambda.min

# Predict the test data with the best lambda
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test, ])

# Assuming predictions are probabilities, convert them to binary classes
predicted_classes <- ifelse(ridge.pred > 0.5, 1, 0)

# Confusion matrix
conf_matrix <- table(predicted_classes, y.test)

# Calculate Sensitivity (True Positive Rate)
sensitivity <- conf_matrix[2, 2] / sum(conf_matrix[2, ])

# Calculate Specificity (True Negative Rate)
specificity <- conf_matrix[1, 1] / sum(conf_matrix[1, ])

# Calculate Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)

# Calculate Precision (Positive Predictive Value)
precision <- conf_matrix[2, 2] / sum(conf_matrix[, 2])

# Calculate Recall (Same as Sensitivity)
recall <- sensitivity

# Calculate False Positive Rate
fpr <- 1 - specificity

# Create ROC Curve
roc_curve <- roc(y.test, ridge.pred)

# Calculate AUC (Area Under Curve)
auc_value <- auc(roc_curve)

# Print the metrics
print("Metrics:")
print(paste("Sensitivity (True Positive Rate):", sensitivity))
print(paste("Specificity (True Negative Rate):", specificity))
print(paste("Accuracy:", accuracy))
print(paste("Precision (Positive Predictive Value):", precision))
print(paste("Recall:", recall))
print(paste("False Positive Rate:", fpr))
print(paste("AUC (Area Under Curve):", auc_value))

# Plot ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue")

coef(ridge.mod)



```

```{r}

# Call lambda values and corresponding test MSEs
lambda_values <- cv.out$lambda
test_mses <- cv.out$cvm

# Plot the test MSE as a function of the log of the regularization 
# parameter (i.e. log($\lambda$)) for several orders of magnitude.

plot(log(lambda_values), test_mses, type = "b", 
     xlab = "log(lambda)", ylab = "Test MSE",
     main = "Test MSE vs. log(lambda) of Ridge Regression")


```

# Lasso

```{r}

library(glmnet)
library(pROC)

# Train Lasso regression model
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda.min.ratio = 0.000001)

# Cross-validation for selecting lambda
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1, lambda.min.ratio = 0.000001)

# Find lambda that minimizes training MSE
bestlam <- cv.out$lambda.min

# Predict the test data with the best lambda
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test, ])

# Assuming predictions are probabilities, convert them to binary classes
predicted_classes <- ifelse(lasso.pred > 0.5, 1, 0)

# Confusion matrix
conf_matrix <- table(predicted_classes, y[test])

# Calculate Sensitivity (True Positive Rate)
sensitivity <- conf_matrix[2, 2] / sum(conf_matrix[2, ])

# Calculate Specificity (True Negative Rate)
specificity <- conf_matrix[1, 1] / sum(conf_matrix[1, ])

# Calculate Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)

# Calculate Precision (Positive Predictive Value)
precision <- conf_matrix[2, 2] / sum(conf_matrix[, 2])

# Calculate Recall (Same as Sensitivity)
recall <- sensitivity

# Calculate False Positive Rate
fpr <- 1 - specificity

# Create ROC Curve
roc_curve <- roc(y[test], lasso.pred)

# Calculate AUC (Area Under Curve)
auc_value <- auc(roc_curve)

# Print the metrics
print("Metrics:")
print(paste("Sensitivity (True Positive Rate):", sensitivity))
print(paste("Specificity (True Negative Rate):", specificity))
print(paste("Accuracy:", accuracy))
print(paste("Precision (Positive Predictive Value):", precision))
print(paste("Recall:", recall))
print(paste("False Positive Rate:", fpr))
print(paste("AUC (Area Under Curve):", auc_value))

# Plot ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue")

coef(lasso.mod)

```

```{r}
library(caret)
library(pROC)
library(glmnet)
library(rpart)

library(e1071)  
library(class) 


evaluate_model <- function(model, actual, predicted_prob, threshold = 0.5, plot_roc = TRUE, show_summary = FALSE) {
  predicted_classes <- ifelse(predicted_prob > threshold, 1, 0)
  confusion <- confusionMatrix(as.factor(predicted_classes), as.factor(actual), positive = "1")


  roc_result <- roc(actual, predicted_prob)
  auc_value <- auc(roc_result)


  if (plot_roc) {
    plot(roc_result, main = "ROC Curve")
    abline(a = 0, b = 1, col = "red")  # Adding a reference line
    text(x = 0.8, y = 0.2, labels = paste("AUC =", round(auc_value, 2)))
  }
  
  # Optionally print model summary
  if (show_summary && !is.null(model)) {
    print(summary(model))
  }
  

  list(
    Accuracy = confusion$overall['Accuracy'],
    Precision = confusion$byClass['Precision'],
    Recall = confusion$byClass['Sensitivity'], 
    Specificity = confusion$byClass['Specificity'],
    AUC = auc_value,
    ROC = roc_result
  )
}

#set.seed(123) # for reproducibility
#train_idx <- createDataPartition(df1$D1A_1V1L_1, p = 0.8, list = FALSE)
#train_set <- df1[train_idx, ]
#test_set <- df1[-train_idx, ]
```

# Elastic

```{r}
library(glmnet)
library(pROC)

# Train Elastic Net regression model
elastic.mod <- glmnet(x[train, ], y[train], alpha = 0.5, lambda.min.ratio = 0.000001)

# Cross-validation for selecting lambda
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0.5, lambda.min.ratio = 0.000001)

# Find lambda that minimizes training MSE
bestlam <- cv.out$lambda.min

# Predict the test data with the best lambda
elastic.pred <- predict(elastic.mod, s = bestlam, newx = x[test, ])

# Assuming predictions are probabilities, convert them to binary classes
predicted_classes <- ifelse(elastic.pred > 0.5, 1, 0)

# Confusion matrix
conf_matrix <- table(predicted_classes, y[test])

# Calculate Sensitivity (True Positive Rate)
sensitivity <- conf_matrix[2, 2] / sum(conf_matrix[2, ])

# Calculate Specificity (True Negative Rate)
specificity <- conf_matrix[1, 1] / sum(conf_matrix[1, ])

# Calculate Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)

# Calculate Precision (Positive Predictive Value)
precision <- conf_matrix[2, 2] / sum(conf_matrix[, 2])

# Calculate Recall (Same as Sensitivity)
recall <- sensitivity

# Calculate False Positive Rate
fpr <- 1 - specificity

# Create ROC Curve
roc_curve <- roc(y[test], elastic.pred)

# Calculate AUC (Area Under Curve)
auc_value <- auc(roc_curve)

# Print the metrics
print("Metrics:")
print(paste("Sensitivity (True Positive Rate):", sensitivity))
print(paste("Specificity (True Negative Rate):", specificity))
print(paste("Accuracy:", accuracy))
print(paste("Precision (Positive Predictive Value):", precision))
print(paste("Recall:", recall))
print(paste("False Positive Rate:", fpr))
print(paste("AUC (Area Under Curve):", auc_value))

# Plot ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue")

coef(elastic.mod)



```

```{r}

library(glmnet)


# Calculate the test MSE
elastic_mse <- mean((elastic.pred - y[test])^2)

# Print and report test error
print("Report Test Error:")
cat("Elastic Net MSE:", elastic_mse, "\n")
print("Elastic Net Regression combines Lasso and Ridge regularization.")

# Plot of non-zero coefficient estimates
plot(elastic.mod, xvar = "lambda")

# Get final model with selected lambda
elastic.mod.final <- glmnet(x[train, ], y[train], alpha = 0.5, lambda = bestlam)

# Sparse matrix
```




From Line 487 in feature_select_r_sheeba.qmd

# Decision Tree

```{r}
#| vscode: {languageId: r}


set.seed(123) # for reproducibility
#train_idx <- createDataPartition(df1$D1A_1V1L, p = 0.8, list = FALSE)
train_idx <- createDataPartition(df1$D1A_1V1L_1, p = 0.8, list = FALSE)
train <- df1[train_idx,]
test <- df1[-train_idx,] 

tree_model <- rpart(D1A_1V1L_1 ~ ., data = train, method = "class")
library(rpart.plot)

rpart.plot(tree_model, main="Decision Tree Model", extra=102)  # extra=102 to show node numbers and splits
predictions_prob <- predict(tree_model, newdata = test, type = "prob")
results <- evaluate_model(tree_model, test$D1A_1V1L_1, predictions_prob[,2], plot_roc = TRUE, show_summary = TRUE)

print(results)
```

# Decision Tree (Hyperparameter Tuned)

```{r}
#| vscode: {languageId: r}
library(rpart)
control <- rpart.control(minsplit = 20, minbucket = 7, maxdepth=30)
fit <- rpart(D1A_1V1L_1 ~ ., data=train, method="class", control=control)
```

```{r}
#| vscode: {languageId: r}
rpart.plot(fit, main="Decision Tree Model (Hyperparameter Tuning)", extra=102)  # extra=102 to show node numbers and splits
predictions_prob_rf <- predict(fit, newdata = test, type = "prob")
results <- evaluate_model(fit, test$D1A_1V1L_1, predictions_prob_rf[,2], plot_roc = TRUE, show_summary = TRUE)
```

# Random Forest

```{r}
#| vscode: {languageId: r}
library(randomForest)
library(datasets)
library(caret)

train$D1A_1V1L_1 <- factor(train$D1A_1V1L_1)
test$D1A_1V1L_1 <- factor(test$D1A_1V1L_1)

rf <- randomForest(D1A_1V1L_1~., data=train, type='classification', proximity=TRUE, importance=TRUE)
print(rf)



rf_predict <- predict(rf, newdata=test, type='prob')
#results_rf <- evaluate_model(rf, test$D1A_1V1L_1, rf_predict, plot_roc = TRUE, show_summary = TRUE)
#results_rf
```


```{r}
library(pROC)

# Extracting the probabilities of the positive class (assuming it's the first class)
prob_positive_class <- rf_predict[, "1"]

# Compute ROC curve
roc_curve <- roc(test$D1A_1V1L_1, prob_positive_class)

# Plot ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue")
# Add AUC value to the plot
text(0.8, 0.2, paste("AUC =", round(auc(roc_curve), 2)), adj = 0)

```

```{r}
# Plot variable importance
varImpPlot(rf, main = "Variable Importance Plot", cex = 0.6, pch = 19)
```


## Don't need this - to delete

```{r}
#| vscode: {languageId: r}
# Define the confusion matrix components
TN <- 1423
FN <- 77
FP <- 23
TP <- 78

# Calculations
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)  # Recall is the same as sensitivity
specificity <- TN / (TN + FP)

# Output the results
cat(sprintf("Accuracy: %f\n", accuracy))
cat(sprintf("Precision: %f\n", precision))
cat(sprintf("Recall (Sensitivity): %f\n", recall))
cat(sprintf("Specificity: %f\n", specificity))

library(pROC)
roc_result <- roc(response = c(rep(0, TN + FP), rep(1, TP + FN)), predictor = c(rep(0, TN), rep(1, FP), rep(1, TP), rep(0, FN)))
auc_value <- auc(roc_result)
cat(sprintf("AUC: %f\n", auc_value))

```
## Don't need this - to delete
```{r}
#| vscode: {languageId: r}
# with mtry
TN <- 1423
FP <- 23
FN <- 74
TP <- 81

# Calculate metrics
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)  # Also known as sensitivity
specificity <- TN / (TN + FP)

# Print the results
cat(sprintf("Accuracy: %f\n", accuracy))
cat(sprintf("Precision: %f\n", precision))
cat(sprintf("Recall (Sensitivity): %f\n", recall))
cat(sprintf("Specificity: %f\n", specificity))
```
## Don't Need This to Delete
```{r}
#| vscode: {languageId: r}
# Simulate predicted probabilities (Not accurate - for conceptual demonstration only)
# Assuming that TP and FN give us a threshold applied at about the median point of the positive class probabilities
simulated_probs <- c(rep(0, TN), rep(1, FP), runif(FN, 0.45, 0.55), runif(TP, 0.45, 0.55))

# Actual labels corresponding to the simulated probabilities
actual_labels <- c(rep(0, TN + FP), rep(1, TP + FN))

# Calculate ROC and AUC
roc_result <- roc(response = actual_labels, predictor = simulated_probs)
auc_value <- auc(roc_result)
plot(roc_result, main = "Simulated ROC Curve (Approximate)")

# Print AUC
cat(sprintf("AUC: %f\n", auc_value))
```

# Random Forest (Hyperparameter Tuned) - Don't Need this

```{r}
#| vscode: {languageId: r}
# # Random Search
# control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
# set.seed(123)
# mtry <- sqrt(ncol(train_set))
# rf_random <- train(D1A_1V9L~., data=train_set, method="rf", tuneLength=15, trControl=control)
# print(rf_random)
# plot(rf_random)
```

```{r}
#| vscode: {languageId: r}
rf <- randomForest(D1A_1V9L~., data=train_set, proximity=TRUE)
print(rf)

rf_predict= predict(rf, newdata=test_set, type='prob')
results_rf <- evaluate_model(tree_model, test_set$D1A_1V9L, rf_predict[,2], plot_roc = TRUE, show_summary = TRUE)
```

```{r}
#| vscode: {languageId: r}
# Define the confusion matrix components
TN <- 1423
FP <- 23
FN <- 74
TP <- 81

# Calculate metrics
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)  # Also known as sensitivity
specificity <- TN / (TN + FP)

# Print the results
cat(sprintf("Accuracy: %f\n", accuracy))
cat(sprintf("Precision: %f\n", precision))
cat(sprintf("Recall (Sensitivity): %f\n", recall))
cat(sprintf("Specificity: %f\n", specificity))
```

```{r}
#| vscode: {languageId: r}
# Load necessary library
if (!require("pROC")) install.packages("pROC")
library(pROC)

# Assuming `pred_probs` is the vector of probabilities that each test instance belongs to class 1
roc_result <- roc(response = as.numeric(c(rep(0, TN + FP), rep(1, TP + FN))), predictor = pred_probs)
auc_value <- auc(roc_result)
plot(roc_result, main = "ROC Curve")

# Print AUC
cat(sprintf("AUC: %f\n", auc_value))
```

```{r}
#| vscode: {languageId: r}
print(rf_random)
```

```{r}
#| vscode: {languageId: r}
rf_predict= predict(rf_random, newdata = test_set, type = "prob") 
results <- evaluate_model(tree_model, test_set$D1A_1V9L, rf_predict[,2], plot_roc = TRUE, show_summary = TRUE)
```

# XG Boost


```{r}
#| vscode: {languageId: r}
library(xgboost)
library(caTools)
library(dplyr)
library(caret)

set.seed(42)

train_idx <- createDataPartition(df1$D1A_1V1L_1, p = 0.8, list = FALSE)
train <- df1[train_idx,]
test <- df1[-train_idx,] 


#sample_split <- sample.split(Y = df1$D1A_1V1L, SplitRatio = 0.7)
#train_set <- subset(x = df1, sample_split == TRUE)
#test_set <- subset(x = df1, sample_split == FALSE)

#y_train <- as.integer(train_set$D1A_1V1L) - 1
#y_test <- as.integer(test_set$D1A_1V1L) - 1
#X_train <- train_set %>% select(-D1A_1V1L)
#X_test <- test_set %>% select(-D1A_1V1L)

# Split the dataset into training and testing sets
X_train <- df1[train_idx, -which(names(df1) == "D1A_1V1L_1")]  # Exclude the target variable from the training set
y_train <- df1[train_idx, "D1A_1V1L_1"]   # Extract the target variable for the training set

X_test <- df1[-train_idx, -which(names(df1) == "D1A_1V1L_1")]  # Exclude the target variable from the testing set
y_test <- df1[-train_idx, "D1A_1V1L_1"]   # Extract the target variable for the testing set


xgb_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
xgb_test <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)
xgb_params <- list(
  booster = "gbtree",
  eta = 0.01,
  max_depth = 2,
  gamma = 4,
  subsample = 0.75,
  colsample_bytree = 1,
  objective = "binary:logistic",
  eval_metric = "mlogloss",
  num_class = length(levels(df1$D1A_1V1L))
)

xgb_model <- xgb.train(
  params = xgb_params,
  data = xgb_train,
  nrounds = 5000,
  verbose = 1
)
xgb_model
```

```{r}
# Make predictions on the test set
predictions <- predict(xgb_model, xgb_test)

# Convert probabilities to class labels, assign greater than 0.5 to Positive
pred_classes <- ifelse(predictions > 0.5, "Positive", "Negative")

# Create the confusion matrix
cm <- table(Actual = test$D1A_1V1L_1, Predicted = pred_classes)

# Confusion Matrix
print(cm)

```
```{r}

# Calculate True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN)
TP <- 319
TN <- 58
FP <- 16
FN <- 7

# Calculate accuracy
accuracy <- (TP + TN) / sum(cm)

# Calculate precision
precision <- TP / (TP + FP)

# Calculate recall (also called sensitivity)
recall <- TP / (TP + FN)

# Calculate F1 score
F1 <- 2 * (precision * recall) / (precision + recall)

# Print the metrics
cat("Accuracy:", accuracy, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1 Score:", F1, "\n")

```



```{r}
#| vscode: {languageId: r}
library(pROC)

pred_probs <- predict(xgb_model, xgb_test)
# Compute ROC curve
roc_curve <- roc(y_test, pred_probs)

# Plot ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue")

auc_value <- auc(roc_curve)
cat(sprintf("AUC: %f", auc_value))
```

```{r}
#| vscode: {languageId: r}

# Get variable importance
importance <- xgb.importance(model = xgb_model)

# Print the variable importance
print(importance)
```


